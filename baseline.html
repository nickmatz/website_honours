<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Baseline Models</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">IsiXhosa Medical MT</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Baseline Models: Transformer and NLLB-200</h1>

							<h2>Overview</h2>
							<p>Baseline models serve as the foundation for evaluating improvements introduced by synthetic data generation methods such as DALI and Back-Translation. This section compares two baseline systems designed for English–isiXhosa medical translation: 
							a <strong>Transformer model trained from scratch</strong> and a <strong>fine-tuned multilingual model (NLLB-200)</strong>. Both were evaluated across general and medical domain datasets to assess how different training strategies affect translation quality.</p>

							<hr />

							<h2>Our Approach</h2>
							<p>The <strong>Transformer baseline</strong> was trained entirely from scratch on the <strong>WMT22</strong> dataset, using approximately 8 million general-domain sentence pairs. This approach learns isiXhosa linguistic structures directly, without prior multilingual knowledge.</p>
							<p>The <strong>NLLB-200 baseline</strong> leveraged transfer learning by fine-tuning a pre-trained multilingual model on the same WMT22 dataset. The aim was to test whether cross-lingual knowledge from other African and low-resource languages could improve translation quality for isiXhosa medical text.</p>
							<p>Both baselines were evaluated on three datasets: <strong>FLORES-200 (general domain)</strong>, <strong>MeMaT (medical validation)</strong>, and <strong>Blocker (medical test set)</strong>. Metrics used included BLEU, chrF, and chrF++.</p>

							<hr />

							<h2>Results</h2>
							<p>The table below presents comparative results for the Transformer and NLLB-200 models across all datasets and translation directions (English→isiXhosa and isiXhosa→English). These scores serve as reference points for evaluating the performance of domain-adapted models.</p>
							
							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th rowspan="2">Dataset</th>
											<th rowspan="2">Model</th>
											<th colspan="3">English → isiXhosa</th>
											<th colspan="3">isiXhosa → English</th>
										</tr>
										<tr>
											<th>BLEU</th>
											<th>chrF</th>
											<th>chrF++</th>
											<th>BLEU</th>
											<th>chrF</th>
											<th>chrF++</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td rowspan="2"><strong>FLORES-200</strong></td>
											<td>Transformer (Scratch)</td>
											<td>15.0</td>
											<td>37.2</td>
											<td>38.5</td>
											<td><strong>26.9</strong></td>
											<td><strong>47.7</strong></td>
											<td><strong>49.3</strong></td>
										</tr>
										<tr>
											<td>NLLB-200 (Fine-tuned)</td>
											<td><strong>15.2</strong></td>
											<td><strong>38.1</strong></td>
											<td><strong>39.4</strong></td>
											<td>21.1</td>
											<td>44.3</td>
											<td>45.8</td>
										</tr>

										<tr>
											<td rowspan="2"><strong>MeMaT (Medical Validation)</strong></td>
											<td>Transformer (Scratch)</td>
											<td>10.5</td>
											<td>33.6</td>
											<td>34.9</td>
											<td>13.2</td>
											<td>38.5</td>
											<td>39.8</td>
										</tr>
										<tr>
											<td>NLLB-200 (Fine-tuned)</td>
											<td><strong>13.8</strong></td>
											<td><strong>35.9</strong></td>
											<td><strong>37.3</strong></td>
											<td><strong>17.5</strong></td>
											<td><strong>40.6</strong></td>
											<td><strong>41.7</strong></td>
										</tr>

										<tr>
											<td rowspan="2"><strong>Blocker (Medical Test)</strong></td>
											<td>Transformer (Scratch)</td>
											<td><strong>12.9</strong></td>
											<td><strong>31.2</strong></td>
											<td><strong>32.8</strong></td>
											<td><strong>14.4</strong></td>
											<td><strong>35.1</strong></td>
											<td><strong>36.5</strong></td>
										</tr>
										<tr>
											<td>NLLB-200 (Fine-tuned)</td>
											<td>2.1</td>
											<td>25.4</td>
											<td>26.9</td>
											<td>5.8</td>
											<td>29.7</td>
											<td>30.8</td>
										</tr>
									</tbody>
								</table>
							</div>

							<h4>Analysis</h4>
							<p>Results reveal distinct strengths between the two baselines. The <strong>NLLB-200 model</strong> achieved higher scores on the general and validation datasets, reflecting its broad multilingual knowledge and ability to generalize across domains. 
							In contrast, the <strong>Transformer trained from scratch</strong> performed significantly better on the Blocker medical test set, indicating stronger domain-specific adaptation when trained solely on in-domain data. 
							Overall, both baselines provided essential benchmarks—demonstrating that while multilingual fine-tuning supports general performance, training from scratch can yield superior results in specialized medical contexts.</p>

							<p>For more detail on architecture, training parameters, and evaluation metrics, please refer to the <a href="#">final paper</a>.</p>
						</div>
					</section>
			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; isiXhosa Medical MT. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
