<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Back-Translation Method</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">IsiXhosa Medical MT</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Back-Translation for Synthetic Data Generation</h1>

							<h2>Overview</h2>
							<p>Due to the scarcity of real parallel medical corpora for English-isiXhosa, synthetic data generation is a crucial technique to bridge this resource gap. This project explored two synthetic data approaches, finding that <strong>back-translation was the most successful</strong>. It consistently produced better semantic and word-level accuracy than the DALI method, suggesting that back-translation is a highly viable approach for medical English-isiXhosa Neural Machine Translation (NMT) tasks. To further test the limits of this approach, we used large-scale NLLB models as a base for fine-tuning, specifically training on the 1.3 billion parameter version to investigate whether scaling up the base model would lead to greater improvements in translation quality.</p>

							<hr />

							<h2>Our Approach</h2>
							<p>While DALI offers one method, back-translation provides a powerful alternative. The core idea is to start with high-quality monolingual text in the target language (in our case, English) and use an existing translation model to generate synthetic source text (isiXhosa). This creates a "pseudo-parallel" corpus that can be used to train a new, more accurate model.</p>
							
							<!-- Changed Section: Image and Text side-by-side -->
							<div class="split style1">
								<section>
									<a href="#" class="image"><img src="images/fwd.png" alt="A diagram showing the forward and back-translation data generation pipeline." style="max-width: 80%; margin: auto; display: block;" /></a>
								</section>
								<section>
									<p>The process, illustrated here, begins with monolingual English medical text (a). This text is fed into a base NLLB-200 model (b) to produce synthetic isiXhosa translations (c). These initial stages are identical for both forward- and back-translation.</p>
									<p>The crucial difference occurs at the pairing stage (d). For <strong>back-translation</strong>, the synthetic isiXhosa is paired with the original, real English text, creating a `(synthetic isiXhosa, real English)` corpus. This corpus is then used to fine-tune an isiXhosa-to-English NLLB model. This ordering is a key advantage, as having clean, real English sentences on the target side provides the model with a much more reliable and accurate training signal.</p>
								</section>
							</div>
							
							<hr />

							<h2>Results</h2>
							<p>The models were evaluated using chrF++, BLEU, and ACoM metrics. While chrF++ and BLEU focus on word-level overlap, ACoM (AfriCOMET) is a semantic metric designed to evaluate meaning preservation, which is critical in a medical context. The table below shows the validation results for models fine-tuned on back-translated data compared to their respective base models.</p>
							
							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th rowspan="2">Model Base</th>
											<th rowspan="2">Set</th>
											<th rowspan="2">Model</th>
											<th colspan="3">English → isiXhosa</th>
											<th colspan="3">isiXhosa → English</th>
										</tr>
										<tr>
											<th>chrF++</th>
											<th>BLEU</th>
											<th>ACoM</th>
											<th>chrF++</th>
											<th>BLEU</th>
											<th>ACoM</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td rowspan="4"><strong>NLLB 600M</strong></td>
											<td rowspan="2">FLORES-200</td>
											<td>Base-NLLB</td>
											<td>46.56</td>
											<td>14.19</td>
											<td>0.67</td>
											<td>55.46</td>
											<td>34.01</td>
											<td>0.65</td>
										</tr>
										<tr>
											<td>FLORES-Best</td>
											<td>47.01 (<strong>+0.45</strong>)</td>
											<td>15.85 (<strong>+1.66</strong>)</td>
											<td>0.69 (<strong>+0.02</strong>)</td>
											<td>56.93 (<strong>+1.47</strong>)</td>
											<td>35.72 (<strong>+1.71</strong>)</td>
											<td>0.60 (-0.05)</td>
										</tr>
										<tr>
											<td rowspan="2">MeMaT</td>
											<td>Base-NLLB</td>
											<td>41.54</td>
											<td>7.97</td>
											<td>0.97</td>
											<td>39.29</td>
											<td>21.07</td>
											<td>0.78</td>
										</tr>
										<tr>
											<td>MeMaT-Best</td>
											<td>43.84 (<strong>+2.30</strong>)</td>
											<td>8.96 (<strong>+0.99</strong>)</td>
											<td>0.84 (-0.13)</td>
											<td>41.03 (<strong>+1.74</strong>)</td>
											<td>24.58 (<strong>+3.51</strong>)</td>
											<td>0.80 (<strong>+0.02</strong>)</td>
										</tr>
										<tr>
											<td colspan="9"></td>
										</tr>
										<tr>
											<td rowspan="4"><strong>NLLB 1.3B</strong></td>
											<td rowspan="2">FLORES-200</td>
											<td>Base-NLLB</td>
											<td>47.31</td>
											<td>14.81</td>
											<td>0.67</td>
											<td>58.49</td>
											<td>37.97</td>
											<td>0.76</td>
										</tr>
										<tr>
											<td>FLORES-Best</td>
											<td>47.73 (<strong>+0.42</strong>)</td>
											<td>15.41 (<strong>+0.60</strong>)</td>
											<td>0.68 (<strong>+0.01</strong>)</td>
											<td>59.44 (<strong>+0.95</strong>)</td>
											<td>39.25 (<strong>+1.28</strong>)</td>
											<td>0.77 (<strong>+0.01</strong>)</td>
										</tr>
										<tr>
											<td rowspan="2">MeMaT</td>
											<td>Base-NLLB</td>
											<td>42.99</td>
											<td>10.38</td>
											<td>0.84</td>
											<td>43.36</td>
											<td>25.12</td>
											<td>0.78</td>
										</tr>
										<tr>
											<td>MeMaT-Best</td>
											<td>43.74 (<strong>+0.75</strong>)</td>
											<td>9.28 (-1.10)</td>
											<td>0.84 (±0.00)</td>
											<td>44.08 (<strong>+0.72</strong>)</td>
											<td>27.80 (<strong>+2.68</strong>)</td>
											<td>0.80 (<strong>+0.02</strong>)</td>
										</tr>
									</tbody>
								</table>
							</div>
							
							<h4>Analysis</h4>
							<p>For the <strong>600M parameter model</strong>, forward-translation (English → isiXhosa) shows modest but consistent improvements in chrF++ and BLEU scores, outperforming the DALI approach. The gains from back-translation (isiXhosa → English) are more substantial, with significant improvements across nearly all metrics.</p>
							<p>The <strong>1.3B parameter model</strong> results highlight the strength of back-translation even further. While forward-translation gains did not scale with the increased model size, the back-translation model continued to show strong improvements. Its chrF++ scores began to approach those of commercial translation systems, demonstrating the effectiveness of this technique in a low-resource medical context.</p>

							<p>For a full breakdown of all results and a deeper dive into related works, please see our <a href="#">final research paper</a> and the accompanying <a href="#">literature review</a>.</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
