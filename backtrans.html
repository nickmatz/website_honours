<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Back-Translation Method</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">IsiXhosa Medical MT</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Back-Translation for Synthetic Data Generation</h1>

							<h2>Overview</h2>
							<p>Due to the scarcity of real parallel medical corpora for English-isiXhosa, synthetic data generation is a crucial technique to bridge this resource gap. This project explored two synthetic data approaches, finding that <strong>back-translation was the most successful</strong>. It consistently produced better semantic and word-level accuracy than the DALI method, suggesting that back-translation is a highly viable approach for medical English-isiXhosa Neural Machine Translation (NMT) tasks. To further test the limits of this approach, we used large-scale NLLB models as a base for fine-tuning, specifically training on the 1.3 billion parameter version to investigate whether scaling up the base model would lead to greater improvements in translation quality.</p>

							<hr />

							<h2>Our Approach</h2>
							<p>While DALI offers one method, back-translation provides a powerful alternative. The core idea is to start with high-quality monolingual text in the target language (in our case, English) and use an existing translation model to generate synthetic source text (isiXhosa). This creates a "pseudo-parallel" corpus that can be used to train a new, more accurate model.</p>
							
							<div class="split style1">
								<section>
									<a href="#" class="image"><img src="images/fwd.png" alt="A diagram showing the forward and back-translation data generation pipeline." style="max-width: 80%; margin: auto; display: block;" /></a>
								</section>
								<section>
									<p>The process, illustrated here, begins with monolingual English medical text (a). This text is fed into a base NLLB-200 model (b) to produce synthetic isiXhosa translations (c). These initial stages are identical for both forward- and back-translation.</p>
									<p>The crucial difference occurs at the pairing stage (d). For <strong>back-translation</strong>, the synthetic isiXhosa is paired with the original, real English text, creating a `(synthetic isiXhosa, real English)` corpus. This corpus is then used to fine-tune an isiXhosa-to-English NLLB model. This ordering is a key advantage, as having clean, real English sentences on the target side provides the model with a much more reliable and accurate training signal.</p>
								</section>
							</div>
							
							<hr />

							<h2>Results</h2>
							<p>The models were evaluated using chrF++, BLEU, and ACoM metrics. While chrF++ and BLEU focus on word-level overlap, ACoM (AfriCOMET) is a semantic metric designed to evaluate meaning preservation, which is critical in a medical context. The table below shows the Blocker test set results for models fine-tuned on forward- and back-translated data compared to their respective base models and team member's models.</p>
							
							<!-- UPDATED TABLE SECTION -->
							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th rowspan="2">Model Category</th>
											<th rowspan="2">Model</th>
											<th colspan="3">English → isiXhosa</th>
											<th colspan="3">isiXhosa → English</th>
										</tr>
										<tr>
											<th>chrF++</th>
											<th>BLEU</th>
											<th>ACoM</th>
											<th>chrF++</th>
											<th>BLEU</th>
											<th>ACoM</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td rowspan="5"><strong>NLLB-200 600M</strong></td>
											<td>Base-NLLB</td>
											<td>42.27</td>
											<td>15.78</td>
											<td>0.27</td>
											<td>42.50</td>
											<td>26.87</td>
											<td><strong>0.77</strong></td>
										</tr>
										<tr>
											<td>FLORES-Best</td>
											<td><strong>43.80 (+1.53)</strong></td>
											<td><strong>17.86 (+2.08)</strong></td>
											<td>0.67 (+0.40)</td>
											<td><strong>48.05 (+5.55)</strong></td>
											<td>34.46 (+7.59)</td>
											<td>0.71 (–0.06)</td>
										</tr>
										<tr>
											<td>MeMaT-Best</td>
											<td>42.57 (+0.30)</td>
											<td>17.53 (+1.75)</td>
											<td><strong>0.81 (+0.54)</strong></td>
											<td>47.97 (+5.47)</td>
											<td><strong>34.48 (+7.61)</strong></td>
											<td>0.75 (–0.02)</td>
										</tr>
										<tr>
											<td>DALI</td>
											<td>42.08 (–0.19)</td>
											<td>9.72 (–6.06)</td>
											<td>0.35 (+0.08)</td>
											<td>39.13 (–3.37)</td>
											<td>19.03 (–7.84)</td>
											<td>0.56 (–0.21)</td>
										</tr>
										<tr>
											<td>WMT22-NLLB</td>
											<td>3.21 (–39.06)</td>
											<td>2.08 (–13.70)</td>
											<td>0.66 (+0.39)</td>
											<td>26.49 (–16.01)</td>
											<td>5.83 (–21.04)</td>
											<td>0.77 (0.00)</td>
										</tr>
										<tr>
											<td colspan="8"></td>
										</tr>
										<tr>
											<td rowspan="3"><strong>NLLB-200 1.3B</strong></td>
											<td>Base-NLLB</td>
											<td><strong>44.86</strong></td>
											<td><strong>17.70</strong></td>
											<td><strong>0.87</strong></td>
											<td>46.02</td>
											<td>30.07</td>
											<td><strong>0.77</strong></td>
										</tr>
										<tr>
											<td>FLORES-Best</td>
											<td>42.76 (-2.10)</td>
											<td>15.61 (-2.09)</td>
											<td>0.65 (-0.22)</td>
											<td>50.37 (+4.35)</td>
											<td><strong>36.07 (+6.00)</strong></td>
											<td>0.73 (-0.04)</td>
										</tr>
										<tr>
											<td>MeMaT-Best</td>
											<td>42.60 (-2.26)</td>
											<td>15.60 (-2.10)</td>
											<td>0.70 (-0.17)</td>
											<td><strong>50.41 (+4.39)</strong></td>
											<td>35.98 (+5.91)</td>
											<td>0.75 (-0.02)</td>
										</tr>
										<tr>
											<td colspan="8"></td>
										</tr>
										<tr>
											<td rowspan="2"><strong>Commercial</strong></td>
											<td>Google Cloud Trans</td>
											<td>63.79</td>
											<td>28.40</td>
											<td>—</td>
											<td>57.23</td>
											<td>28.60</td>
											<td>—</td>
										</tr>
										<tr>
											<td>ChatGPT (mod)</td>
											<td>52.38</td>
											<td>11.40</td>
											<td>—</td>
											<td>57.59</td>
											<td>26.70</td>
											<td>—</td>
										</tr>
									</tbody>
								</table>
							</div>
							
							<h4>Analysis</h4>
							<p>For the <strong>600M parameter model</strong>, forward-translation (English → isiXhosa) shows modest but consistent improvements in chrF++ and BLEU scores, outperforming the DALI approach. The gains from back-translation (isiXhosa → English) are more substantial, with significant improvements across nearly all metrics.</p>
							<p>The <strong>1.3B parameter model</strong> results highlight the strength of back-translation even further. While forward-translation gains did not scale with the increased model size, the back-translation model continued to show strong improvements. Its chrF++ scores began to approach those of commercial translation systems, demonstrating the effectiveness of this technique in a low-resource medical context.</p>

							<p>For a full breakdown of all results and a deeper dive into related works, please see our <a href="#">final research paper</a> and the accompanying <a href="#">literature review</a>.</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
